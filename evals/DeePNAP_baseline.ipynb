{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../training')\n",
    "from loader import EOS_TOKEN, PAD_TOKEN\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy.stats import pearsonr\n",
    "import copy\n",
    "import random\n",
    "\n",
    "protein_mapping = {\n",
    "    'D': [0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
    "    'E': [0, 0, 1, 0, 1, 1, 1, 0, 0],\n",
    "    'K': [0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
    "    'R': [0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
    "    'H': [0, 1, 0, 0, 1, 1, 1, 0, 0],\n",
    "    'S': [0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
    "    'T': [0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
    "    'N': [0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
    "    'Q': [0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
    "    'Y': [0, 1, 1, 1, 0, 0, 0, 1, 0],\n",
    "    'G': [1, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "    'A': [1, 0, 0, 1, 0, 1, 0, 0, 1],\n",
    "    'V': [1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
    "    'L': [1, 0, 1, 0, 0, 1, 0, 0, 1],\n",
    "    'I': [1, 0, 1, 0, 1, 0, 0, 0, 1],\n",
    "    'M': [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "    'F': [1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
    "    'W': [1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
    "    'P': [1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
    "    'C': [1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
    "}\n",
    "\n",
    "nucleotide_mapping = {\n",
    "    'A': [1, 0, 0, 0, 0],\n",
    "    'C': [0, 1, 0, 0, 0],\n",
    "    'G': [0, 0, 1, 0, 0],\n",
    "    'T': [0, 0, 0, 1, 0],\n",
    "    'U': [0, 0, 0, 0, 1]\n",
    "}\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeePNAP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeePNAP, self).__init__()\n",
    "        self.protein_conv1 = nn.Conv2d(1, 48, (6, 9), stride=(6, 1))\n",
    "        self.protein_conv2 = nn.Conv2d(1, 48, (6, 9), stride=(6, 1))\n",
    "\n",
    "        self.dna_conv1 = nn.Conv2d(1, 32, (2, 5), stride=(2, 1))\n",
    "        self.dna_conv2 = nn.Conv2d(1, 32, (2, 5), stride=(2, 1))\n",
    "\n",
    "        self.interaction_layer1_p1_n1 = nn.Linear(168 + 39, 96)\n",
    "        self.interaction_layer2_p1_n1 = nn.Linear(96, 32)\n",
    "\n",
    "        self.interaction_layer1_p1_n2 = nn.Linear(168 + 39, 96)\n",
    "        self.interaction_layer2_p1_n2 = nn.Linear(96, 32)\n",
    "\n",
    "        self.interaction_layer1_p2_n1 = nn.Linear(168 + 39, 96)\n",
    "        self.interaction_layer2_p2_n1 = nn.Linear(96, 32)\n",
    "\n",
    "        self.interaction_layer1_p2_n2 = nn.Linear(168 + 39, 96)\n",
    "        self.interaction_layer2_p2_n2 = nn.Linear(96, 32)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, protein, nucleotide):\n",
    "        p1 = F.relu(self.protein_conv1(F.pad(protein, (0, 0, 0, 8))))\n",
    "        p1 = p1.transpose(3, 1)\n",
    "        p1 = F.max_pool2d(p1, (1, 48))\n",
    "        p1 = p1.flatten(start_dim=1)\n",
    "\n",
    "        # pad protein with \n",
    "        p2 = F.relu(self.protein_conv2(F.pad(protein, (0, 0, 8, 0))))\n",
    "        p2 = p2.transpose(3, 1)\n",
    "        p2 = F.max_pool2d(p2, (1, 48))\n",
    "        p2 = p2.flatten(start_dim=1)\n",
    "\n",
    "        n1 = F.relu(self.dna_conv1(F.pad(nucleotide, (0, 0, 0, 3))))\n",
    "        n1 = n1.transpose(3, 1)\n",
    "        n1 = F.max_pool2d(n1, (1, 32))\n",
    "        n1 = n1.flatten(start_dim=1)\n",
    "\n",
    "        n2 = F.relu(self.dna_conv2(F.pad(nucleotide, (0, 0, 3, 0))))\n",
    "        n2 = n2.transpose(3, 1)\n",
    "        n2 = F.max_pool2d(n2, (1, 32))\n",
    "        n2 = n2.flatten(start_dim=1)\n",
    "\n",
    "        p1_n1 = torch.cat((p1, n1), 1)\n",
    "        p1_n2 = torch.cat((p1, n2), 1)\n",
    "        p2_n1 = torch.cat((p2, n1), 1)\n",
    "        p2_n2 = torch.cat((p2, n2), 1)\n",
    "\n",
    "        p1_n1 = F.leaky_relu(self.interaction_layer1_p1_n1(p1_n1))\n",
    "        p1_n1 = F.leaky_relu(self.interaction_layer2_p1_n1(p1_n1))\n",
    "\n",
    "        p1_n2 = F.leaky_relu(self.interaction_layer1_p1_n2(p1_n2))\n",
    "        p1_n2 = F.leaky_relu(self.interaction_layer2_p1_n2(p1_n2))\n",
    "\n",
    "        p2_n1 = F.leaky_relu(self.interaction_layer1_p2_n1(p2_n1))\n",
    "        p2_n1 = F.leaky_relu(self.interaction_layer2_p2_n1(p2_n1))\n",
    "\n",
    "        p2_n2 = F.leaky_relu(self.interaction_layer1_p2_n2(p2_n2))\n",
    "        p2_n2 = F.leaky_relu(self.interaction_layer2_p2_n2(p2_n2))\n",
    "\n",
    "        x = torch.cat((p1_n1, p1_n2, p2_n1, p2_n2), 1)\n",
    "\n",
    "        res = F.leaky_relu(self.fc1(x))\n",
    "        res = self.dropout(res)\n",
    "        res = F.leaky_relu(self.fc2(res))\n",
    "        res = self.dropout(res)\n",
    "        x = x + res\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, noise_floor=1e-15):\n",
    "    nucleotide_sequences = []\n",
    "    peptides = []\n",
    "\n",
    "    Kd = []\n",
    "    G0 = []\n",
    "\n",
    "    for key in dataset:\n",
    "        for item in dataset[key][\"binding data\"]:\n",
    "            if item[2] == 0 or item[3] == 0:\n",
    "                continue\n",
    "            nucleotide_sequence = item[0]\n",
    "            if item[1] == \"RNA\":\n",
    "                nucleotide_sequence = \"<RNA>\" + nucleotide_sequence + \"<EOS>\"\n",
    "            else:\n",
    "                nucleotide_sequence = \"<DNA>\" + nucleotide_sequence + \"<EOS>\"\n",
    "            \n",
    "            nucleotide_sequences.append(nucleotide_sequence)\n",
    "            peptides.append(dataset[key]['Sequence']) # we exclude the tags here because they are added in prepare_sample after mutation\n",
    "\n",
    "            Kd.append(np.log10(item[2] + noise_floor*np.random.uniform() + noise_floor))\n",
    "            G0.append(item[3])\n",
    "\n",
    "    return nucleotide_sequences, peptides, Kd, G0\n",
    "\n",
    "def encode_peptide(peptide):\n",
    "    peptide = peptide.upper()\n",
    "    peptide = [protein_mapping[aa] for aa in peptide]\n",
    "    peptide = np.asarray(peptide, dtype=np.float32)\n",
    "\n",
    "    # ensure shape is (1, 1, 1000, 9), padding with zeros if necessary\n",
    "    if peptide.shape[0] < 1000:\n",
    "        peptide = np.concatenate((peptide, np.zeros((1000 - peptide.shape[0], 9))), axis=0)\n",
    "    elif peptide.shape[0] > 1000:\n",
    "        peptide = peptide[:1000]\n",
    "\n",
    "    peptide = torch.tensor(peptide).unsqueeze(0).unsqueeze(0)\n",
    "    return peptide\n",
    "\n",
    "def encode_nucleotide_sequence(nucleotide_sequence):\n",
    "    nucleotide_sequence = nucleotide_sequence.upper()\n",
    "    nucleotide_sequence = [nucleotide_mapping[nt] for nt in nucleotide_sequence]\n",
    "    nucleotide_sequence = np.asarray(nucleotide_sequence, dtype=np.float32)\n",
    "\n",
    "    # ensure shape is (1, 1, 75, 5), padding with zeros if necessary\n",
    "    if nucleotide_sequence.shape[0] < 75:\n",
    "        nucleotide_sequence = np.concatenate((nucleotide_sequence, np.zeros((75 - nucleotide_sequence.shape[0], 5))), axis=0)\n",
    "    elif nucleotide_sequence.shape[0] > 75:\n",
    "        nucleotide_sequence = nucleotide_sequence[:75]\n",
    "\n",
    "    nucleotide_sequence = torch.tensor(nucleotide_sequence).unsqueeze(0).unsqueeze(0)\n",
    "    return nucleotide_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random number generators\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# seed torch cuda\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Pre-train\" on no mutation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 92 proteins from the train set\n"
     ]
    }
   ],
   "source": [
    "with open(f'../datasets/pronab_no_mutations.json', 'r') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "training_tuples = [(train_set[key]['Sequence'], # Peptide sequence\n",
    "                    [item[0] for item in train_set[key]['binding data']], # Nucleotide sequence\n",
    "                    [item[3] for item in train_set[key]['binding data']], # G0\n",
    "                    [item[1] for item in train_set[key]['binding data']]) for key in train_set] # RNA/DNA\n",
    "\n",
    "\n",
    "############ DECONTAMINATE TRAIN SET ############\n",
    "test_dataset = []\n",
    "with open('../datasets/mutation_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_dataset.append(json.loads(line))\n",
    "\n",
    "sequences = [test_dataset[i][\"peptide_sequence\"] for i in range(len(test_dataset))]\n",
    "\n",
    "# group by peptide sequence\n",
    "grouped_sequences = {}\n",
    "for i in range(len(test_dataset)):\n",
    "    if test_dataset[i][\"peptide_sequence\"] not in grouped_sequences:\n",
    "        grouped_sequences[test_dataset[i][\"peptide_sequence\"]] = []\n",
    "    grouped_sequences[test_dataset[i][\"peptide_sequence\"]].append(test_dataset[i])\n",
    "\n",
    "# remove sequences that are in the training set\n",
    "deleted = 0\n",
    "for i in range(len(training_tuples)-1, -1, -1):\n",
    "    if training_tuples[i][0] in grouped_sequences:\n",
    "        del training_tuples[i]\n",
    "        deleted += 1\n",
    "print(f\"Deleted {deleted} proteins from the train set\")\n",
    "###############################################\n",
    "\n",
    "peptide_sequences = []\n",
    "nucleotide_sequences = []\n",
    "G0 = []\n",
    "for i in range(len(training_tuples)):\n",
    "    for j in range(len(training_tuples[i][1])):\n",
    "        nucleotide_sequence = training_tuples[i][1][j]\n",
    "\n",
    "        if len(nucleotide_sequence) == 0:\n",
    "            continue\n",
    "\n",
    "        if training_tuples[i][3][j] == \"RNA\":\n",
    "            nucleotide_sequence.replace(\"T\", \"U\")\n",
    "\n",
    "        peptide_sequences.append(training_tuples[i][0])\n",
    "        nucleotide_sequences.append(nucleotide_sequence)\n",
    "        G0.append(training_tuples[i][2][j])\n",
    "\n",
    "G0_mean = np.mean(G0)\n",
    "G0_std = np.std(G0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10882, 660)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(peptide_sequences), len(set(peptide_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5643: 100%|██████████| 1360/1360 [03:09<00:00,  7.17it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 32\n",
    "batch_size = 256\n",
    "\n",
    "model = DeePNAP().to(device).to(dtype)\n",
    "model.train()\n",
    "\n",
    "num_steps = int(num_epochs*len(peptide_sequences)/batch_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=num_steps)\n",
    "\n",
    "pbar = tqdm(range(num_steps))\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if len(nucleotide_sequences[i]) == 0 or len(peptide_sequences[i]) == 0:\n",
    "        continue\n",
    "\n",
    "    indices = np.random.choice(len(peptide_sequences), batch_size, replace=False)\n",
    "\n",
    "    X_protein = [encode_peptide(peptide_sequences[i]) for i in indices]\n",
    "    X_wild = [encode_nucleotide_sequence(nucleotide_sequences[i]) for i in indices]\n",
    "\n",
    "    X_protein = torch.cat(X_protein, dim=0).to(device, dtype)\n",
    "    X_wild = torch.cat(X_wild, dim=0).to(device, dtype)\n",
    "\n",
    "    G0_wild_ground_truths = torch.tensor([(G0[i] - G0_mean) / G0_std for i in indices], dtype=dtype).to(device)\n",
    "    G0_wild = model(X_protein, X_wild).reshape(-1) * G0_std + G0_mean\n",
    "\n",
    "\\\n",
    "    loss = ((G0_wild - G0_wild_ground_truths)**2).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    pbar.set_description(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PCC: 0.0134\n",
      "Test MAE: 1.0658\n",
      "dG PCC: 0.2178\n",
      "dG MAE: 9.5376\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "base_model = copy.deepcopy(model)\n",
    "\n",
    "test_dataset = []\n",
    "with open('../datasets/mutation_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_dataset.append(json.loads(line))\n",
    "\n",
    "sequences = [test_dataset[i][\"peptide_sequence\"] for i in range(len(test_dataset))]\n",
    "\n",
    "random.shuffle(sequences)\n",
    "\n",
    "# group by peptide sequence\n",
    "grouped_sequences = {}\n",
    "for i in range(len(test_dataset)):\n",
    "    if test_dataset[i][\"peptide_sequence\"] not in grouped_sequences:\n",
    "        grouped_sequences[test_dataset[i][\"peptide_sequence\"]] = []\n",
    "    grouped_sequences[test_dataset[i][\"peptide_sequence\"]].append(test_dataset[i])\n",
    "\n",
    "last_test_pcc = 0.0\n",
    "MAE = 0.0\n",
    "\n",
    "last_dG_pcc = 0.0\n",
    "dG_MAE = 0.0\n",
    "\n",
    "all_test_pccs = []\n",
    "all_MAEs = []\n",
    "\n",
    "all_dG_pccs = []\n",
    "all_dG_MAEs = []\n",
    "\n",
    "test_set = []\n",
    "for i, key in enumerate(grouped_sequences.keys()):\n",
    "    test_set += grouped_sequences[key]\n",
    "\n",
    "model = copy.deepcopy(base_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "\n",
    "    dG_predictions = []\n",
    "    dG_ground_truths = []\n",
    "    \n",
    "    for i in range(0, len(test_set)):\n",
    "        X_protein = encode_peptide(test_set[i][\"peptide_sequence\"]).to(device, dtype)\n",
    "        X_wild = encode_nucleotide_sequence(test_set[i][\"wild_nucleotide_sequence\"]).to(device, dtype)\n",
    "        X_mutated = encode_nucleotide_sequence(test_set[i][\"mutated_nucleotide_sequence\"]).to(device, dtype)\n",
    "\n",
    "        G0_wild = model(X_protein, X_wild).reshape(-1).item() * G0_std + G0_mean\n",
    "        G0_mutated = model(X_protein, X_mutated).reshape(-1).item() * G0_std + G0_mean\n",
    "\n",
    "        dG_predictions.extend([G0_wild, G0_mutated])\n",
    "        dG_ground_truths.extend([test_set[i][\"wild_G0\"], test_set[i][\"mutant_G0\"]])\n",
    "\n",
    "        difference = G0_mutated - G0_wild\n",
    "        ground_truth_difference = test_set[i][\"mutant_G0\"] - test_set[i][\"wild_G0\"]\n",
    "\n",
    "        ground_truths.append(ground_truth_difference)\n",
    "        predictions.append(difference)\n",
    "\n",
    "    ground_truths = np.asarray(ground_truths)\n",
    "    predictions = np.asarray(predictions)\n",
    "\n",
    "    dG_ground_truths = np.asarray(dG_ground_truths)\n",
    "    dG_predictions = np.asarray(dG_predictions)\n",
    "\n",
    "    last_test_pcc = pearsonr(ground_truths, predictions)[0]\n",
    "    MAE = np.abs(ground_truths - predictions).mean()\n",
    "\n",
    "    last_dG_pcc = pearsonr(dG_ground_truths, dG_predictions)[0]\n",
    "    dG_MAE = np.abs(dG_ground_truths - dG_predictions).mean()\n",
    "\n",
    "    all_test_pccs.append(last_test_pcc)\n",
    "    all_MAEs.append(MAE)\n",
    "\n",
    "    all_dG_pccs.append(last_dG_pcc)\n",
    "    all_dG_MAEs.append(dG_MAE)\n",
    "\n",
    "    # print results\n",
    "    print(f\"Test PCC: {last_test_pcc:.4f}\")\n",
    "    print(f\"Test MAE: {MAE:.4f}\")\n",
    "    print(f\"dG PCC: {last_dG_pcc:.4f}\")\n",
    "    print(f\"dG MAE: {dG_MAE:.4f}\")\n",
    "    print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Mutation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.3251, last pcc: 0.1645, MAE: 0.6162, dG pcc: 0.5514, dG MAE: 0.5768: 100%|██████████| 706/706 [01:54<00:00,  6.15it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0:\n",
      "Test PCC: 0.1645\n",
      "Test MAE: 0.6162\n",
      "dG PCC: 0.5514\n",
      "dG MAE: 0.5768\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.8128, last pcc: 0.2537, MAE: 0.8232, dG pcc: 0.9255, dG MAE: 0.4986: 100%|██████████| 706/706 [01:57<00:00,  6.03it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "Test PCC: 0.2537\n",
      "Test MAE: 0.8232\n",
      "dG PCC: 0.9255\n",
      "dG MAE: 0.4986\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.5676, last pcc: 0.3426, MAE: 1.2458, dG pcc: 0.8923, dG MAE: 0.7201: 100%|██████████| 706/706 [01:54<00:00,  6.18it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2:\n",
      "Test PCC: 0.3426\n",
      "Test MAE: 1.2458\n",
      "dG PCC: 0.8923\n",
      "dG MAE: 0.7201\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.3569, last pcc: 0.2697, MAE: 1.1660, dG pcc: 0.5871, dG MAE: 0.7438: 100%|██████████| 706/706 [02:09<00:00,  5.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3:\n",
      "Test PCC: 0.2697\n",
      "Test MAE: 1.1660\n",
      "dG PCC: 0.5871\n",
      "dG MAE: 0.7437\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.4169, last pcc: 0.4737, MAE: 1.0278, dG pcc: 0.7575, dG MAE: 1.0210: 100%|██████████| 706/706 [02:13<00:00,  5.28it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 4:\n",
      "Test PCC: 0.4737\n",
      "Test MAE: 1.0278\n",
      "dG PCC: 0.7575\n",
      "dG MAE: 1.0210\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.9946, last pcc: 0.3745, MAE: 0.7129, dG pcc: 0.9534, dG MAE: 0.4457: 100%|██████████| 706/706 [02:10<00:00,  5.40it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5:\n",
      "Test PCC: 0.3745\n",
      "Test MAE: 0.7129\n",
      "dG PCC: 0.9534\n",
      "dG MAE: 0.4456\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.4189, last pcc: 0.6268, MAE: 1.2289, dG pcc: 0.8988, dG MAE: 0.7407: 100%|██████████| 706/706 [02:08<00:00,  5.48it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 6:\n",
      "Test PCC: 0.6268\n",
      "Test MAE: 1.2289\n",
      "dG PCC: 0.8988\n",
      "dG MAE: 0.7407\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.4160, last pcc: 0.7047, MAE: 0.9525, dG pcc: 0.8641, dG MAE: 0.5649: 100%|██████████| 706/706 [02:14<00:00,  5.26it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 7:\n",
      "Test PCC: 0.7047\n",
      "Test MAE: 0.9525\n",
      "dG PCC: 0.8641\n",
      "dG MAE: 0.5649\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.6555, last pcc: 0.2815, MAE: 0.9943, dG pcc: 0.8914, dG MAE: 0.6115: 100%|██████████| 706/706 [02:07<00:00,  5.55it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 8:\n",
      "Test PCC: 0.2815\n",
      "Test MAE: 0.9943\n",
      "dG PCC: 0.8914\n",
      "dG MAE: 0.6115\n",
      "=============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.3119, last pcc: 0.4261, MAE: 0.8794, dG pcc: 0.9284, dG MAE: 0.4874: 100%|██████████| 706/706 [02:09<00:00,  5.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 9:\n",
      "Test PCC: 0.4261\n",
      "Test MAE: 0.8794\n",
      "dG PCC: 0.9284\n",
      "dG MAE: 0.4873\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "base_model = copy.deepcopy(model)\n",
    "\n",
    "num_epochs = 256\n",
    "batch_size = 256\n",
    "\n",
    "test_dataset = []\n",
    "with open('../datasets/mutation_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_dataset.append(json.loads(line))\n",
    "\n",
    "sequences = [test_dataset[i][\"peptide_sequence\"] for i in range(len(test_dataset))]\n",
    "\n",
    "random.shuffle(sequences)\n",
    "\n",
    "# group by peptide sequence\n",
    "grouped_sequences = {}\n",
    "for i in range(len(test_dataset)):\n",
    "    if test_dataset[i][\"peptide_sequence\"] not in grouped_sequences:\n",
    "        grouped_sequences[test_dataset[i][\"peptide_sequence\"]] = []\n",
    "    grouped_sequences[test_dataset[i][\"peptide_sequence\"]].append(test_dataset[i])\n",
    "\n",
    "last_test_pcc = 0.0\n",
    "MAE = 0.0\n",
    "\n",
    "last_dG_pcc = 0.0\n",
    "dG_MAE = 0.0\n",
    "\n",
    "all_test_pccs = []\n",
    "all_MAEs = []\n",
    "\n",
    "all_dG_pccs = []\n",
    "all_dG_MAEs = []\n",
    "\n",
    "for split in range(10):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for i, key in enumerate(grouped_sequences.keys()):\n",
    "        if i % 10 == split:\n",
    "            test_set += grouped_sequences[key]\n",
    "        train_set += grouped_sequences[key]\n",
    "\n",
    "    model = copy.deepcopy(base_model)\n",
    "    model.train()\n",
    "\n",
    "    num_steps = int(num_epochs*len(train_set)/batch_size)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    #scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=num_steps)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=num_steps, pct_start=0.05)\n",
    "\n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = np.random.choice(len(train_set), batch_size, replace=False)\n",
    "\n",
    "        X_protein = [encode_peptide(train_set[i][\"peptide_sequence\"]) for i in indices]\n",
    "        X_wild = [encode_nucleotide_sequence(train_set[i][\"wild_nucleotide_sequence\"]) for i in indices]\n",
    "        X_mutated = [encode_nucleotide_sequence(train_set[i][\"mutated_nucleotide_sequence\"]) for i in indices]\n",
    "\n",
    "        X_protein = torch.cat(X_protein, dim=0).to(device, dtype)\n",
    "        X_wild = torch.cat(X_wild, dim=0).to(device, dtype)\n",
    "        X_mutated = torch.cat(X_mutated, dim=0).to(device, dtype)\n",
    "\n",
    "        G0_wild_ground_truths = torch.tensor([train_set[i][\"wild_G0\"] for i in indices], dtype=dtype).to(device)\n",
    "        G0_mutated_ground_truths = torch.tensor([train_set[i][\"mutant_G0\"] for i in indices], dtype=dtype).to(device)\n",
    "        G0_diff_ground_truths = G0_mutated_ground_truths - G0_wild_ground_truths\n",
    "\n",
    "        G0_wild = model(X_protein, X_wild).reshape(-1) * G0_std + G0_mean\n",
    "        G0_mutated = model(X_protein, X_mutated).reshape(-1) * G0_std + G0_mean\n",
    "\n",
    "        G0_diff = G0_mutated - G0_wild\n",
    "\n",
    "        loss = ((G0_diff - G0_diff_ground_truths)**2).mean() + ((G0_wild - G0_wild_ground_truths)**2).mean() + ((G0_mutated - G0_mutated_ground_truths)**2).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss:.4f}, last pcc: {last_test_pcc:.4f}, MAE: {MAE:.4f}, dG pcc: {last_dG_pcc:.4f}, dG MAE: {dG_MAE:.4f}\")\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                ground_truths = []\n",
    "                predictions = []\n",
    "\n",
    "                dG_predictions = []\n",
    "                dG_ground_truths = []\n",
    "\n",
    "                for i in range(0, len(test_set)):\n",
    "                    X_protein = encode_peptide(test_set[i][\"peptide_sequence\"]).to(device, dtype)\n",
    "                    X_wild = encode_nucleotide_sequence(test_set[i][\"wild_nucleotide_sequence\"]).to(device, dtype)\n",
    "                    X_mutated = encode_nucleotide_sequence(test_set[i][\"mutated_nucleotide_sequence\"]).to(device, dtype)\n",
    "                    \n",
    "                    G0_wild = model(X_protein, X_wild).reshape(-1).item() * G0_std + G0_mean\n",
    "                    G0_mutated = model(X_protein, X_mutated).reshape(-1).item() * G0_std + G0_mean\n",
    "\n",
    "                    dG_predictions.extend([G0_wild, G0_mutated])\n",
    "                    dG_ground_truths.extend([test_set[i][\"wild_G0\"], test_set[i][\"mutant_G0\"]])\n",
    "\n",
    "                    difference = G0_mutated - G0_wild\n",
    "                    ground_truth_difference = test_set[i][\"mutant_G0\"] - test_set[i][\"wild_G0\"]\n",
    "\n",
    "                    ground_truths.append(ground_truth_difference)\n",
    "                    predictions.append(difference)\n",
    "\n",
    "                    pbar.set_description(f\"(Testing {i}/{len(test_set)}) Loss: {loss:.4f}, test pcc: {last_test_pcc:.4f}, test MAE: {MAE:.4f}\")\n",
    "\n",
    "                ground_truths = np.asarray(ground_truths)\n",
    "                predictions = np.asarray(predictions)\n",
    "\n",
    "                dG_ground_truths = np.asarray(dG_ground_truths)\n",
    "                dG_predictions = np.asarray(dG_predictions)\n",
    "\n",
    "                last_test_pcc = pearsonr(ground_truths, predictions)[0]\n",
    "                MAE = np.abs(ground_truths - predictions).mean()\n",
    "\n",
    "                last_dG_pcc = pearsonr(dG_ground_truths, dG_predictions)[0]\n",
    "                dG_MAE = np.abs(dG_ground_truths - dG_predictions).mean()\n",
    "\n",
    "                model.train()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        ground_truths = []\n",
    "        predictions = []\n",
    "\n",
    "        dG_predictions = []\n",
    "        dG_ground_truths = []\n",
    "        \n",
    "        for i in range(0, len(test_set)):\n",
    "            X_protein = encode_peptide(test_set[i][\"peptide_sequence\"]).to(device, dtype)\n",
    "            X_wild = encode_nucleotide_sequence(test_set[i][\"wild_nucleotide_sequence\"]).to(device, dtype)\n",
    "            X_mutated = encode_nucleotide_sequence(test_set[i][\"mutated_nucleotide_sequence\"]).to(device, dtype)\n",
    "\n",
    "            G0_wild = model(X_protein, X_wild).reshape(-1).item() * G0_std + G0_mean\n",
    "            G0_mutated = model(X_protein, X_mutated).reshape(-1).item() * G0_std + G0_mean\n",
    "\n",
    "            dG_predictions.extend([G0_wild, G0_mutated])\n",
    "            dG_ground_truths.extend([test_set[i][\"wild_G0\"], test_set[i][\"mutant_G0\"]])\n",
    "\n",
    "            difference = G0_mutated - G0_wild\n",
    "            ground_truth_difference = test_set[i][\"mutant_G0\"] - test_set[i][\"wild_G0\"]\n",
    "\n",
    "            ground_truths.append(ground_truth_difference)\n",
    "            predictions.append(difference)\n",
    "\n",
    "        ground_truths = np.asarray(ground_truths)\n",
    "        predictions = np.asarray(predictions)\n",
    "\n",
    "        dG_ground_truths = np.asarray(dG_ground_truths)\n",
    "        dG_predictions = np.asarray(dG_predictions)\n",
    "\n",
    "        last_test_pcc = pearsonr(ground_truths, predictions)[0]\n",
    "        MAE = np.abs(ground_truths - predictions).mean()\n",
    "\n",
    "        last_dG_pcc = pearsonr(dG_ground_truths, dG_predictions)[0]\n",
    "        dG_MAE = np.abs(dG_ground_truths - dG_predictions).mean()\n",
    "\n",
    "        all_test_pccs.append(last_test_pcc)\n",
    "        all_MAEs.append(MAE)\n",
    "\n",
    "        all_dG_pccs.append(last_dG_pcc)\n",
    "        all_dG_MAEs.append(dG_MAE)\n",
    "\n",
    "        # print results\n",
    "        print(f\"Split {split}:\")\n",
    "        print(f\"Test PCC: {last_test_pcc:.4f}\")\n",
    "        print(f\"Test MAE: {MAE:.4f}\")\n",
    "        print(f\"dG PCC: {last_dG_pcc:.4f}\")\n",
    "        print(f\"dG MAE: {dG_MAE:.4f}\")\n",
    "        print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test PCC: 0.3918 ± 0.0511\n",
      "Average test MAE: 0.9647 ± 0.0638\n",
      "Average dG PCC: 0.8250 ± 0.0435\n",
      "Average dG MAE: 0.6410 ± 0.0514\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average test PCC: {np.mean(all_test_pccs):.4f} ± {np.std(all_test_pccs)/np.sqrt(10):.4f}\")\n",
    "print(f\"Average test MAE: {np.mean(all_MAEs):.4f} ± {np.std(all_MAEs)/np.sqrt(10):.4f}\")\n",
    "\n",
    "print(f\"Average dG PCC: {np.mean(all_dG_pccs):.4f} ± {np.std(all_dG_pccs)/np.sqrt(10):.4f}\")\n",
    "print(f\"Average dG MAE: {np.mean(all_dG_MAEs):.4f} ± {np.std(all_dG_MAEs)/np.sqrt(10):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 92 proteins from the train set\n"
     ]
    }
   ],
   "source": [
    "with open(f'../datasets/pronab_no_mutations.json', 'r') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "training_tuples = [(train_set[key]['Sequence'], # Peptide sequence\n",
    "                    [item[0] for item in train_set[key]['binding data']], # Nucleotide sequence\n",
    "                    [item[3] for item in train_set[key]['binding data']], # G0\n",
    "                    [item[1] for item in train_set[key]['binding data']]) for key in train_set] # RNA/DNA\n",
    "\n",
    "\n",
    "############ DECONTAMINATE TRAIN SET ############\n",
    "test_dataset = []\n",
    "with open('../datasets/mutation_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        test_dataset.append(json.loads(line))\n",
    "\n",
    "sequences = [test_dataset[i][\"peptide_sequence\"] for i in range(len(test_dataset))]\n",
    "\n",
    "# group by peptide sequence\n",
    "grouped_sequences = {}\n",
    "for i in range(len(test_dataset)):\n",
    "    if test_dataset[i][\"peptide_sequence\"] not in grouped_sequences:\n",
    "        grouped_sequences[test_dataset[i][\"peptide_sequence\"]] = []\n",
    "    grouped_sequences[test_dataset[i][\"peptide_sequence\"]].append(test_dataset[i])\n",
    "\n",
    "# remove sequences that are in the training set\n",
    "deleted = 0\n",
    "for i in range(len(training_tuples)-1, -1, -1):\n",
    "    if training_tuples[i][0] in grouped_sequences:\n",
    "        del training_tuples[i]\n",
    "        deleted += 1\n",
    "print(f\"Deleted {deleted} proteins from the train set\")\n",
    "###############################################\n",
    "\n",
    "peptide_sequences = []\n",
    "nucleotide_sequences = []\n",
    "G0 = []\n",
    "for i in range(len(training_tuples)):\n",
    "    for j in range(len(training_tuples[i][1])):\n",
    "        nucleotide_sequence = training_tuples[i][1][j]\n",
    "\n",
    "        if len(nucleotide_sequence) == 0:\n",
    "            continue\n",
    "\n",
    "        if training_tuples[i][3][j] == \"RNA\":\n",
    "            nucleotide_sequence.replace(\"T\", \"U\")\n",
    "\n",
    "        peptide_sequences.append(training_tuples[i][0])\n",
    "        nucleotide_sequences.append(nucleotide_sequence)\n",
    "        G0.append(training_tuples[i][2][j])\n",
    "\n",
    "G0_mean = np.mean(G0)\n",
    "G0_std = np.std(G0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(train_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757, 14582)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_interactions = 0\n",
    "\n",
    "for key in keys:\n",
    "    total_interactions += len(train_set[key]['binding data'])\n",
    "\n",
    "len(keys), total_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 1412)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = [test_dataset[i]['peptide_sequence'] for i in range(len(test_dataset))]\n",
    "len(set(test_sequences)), len(test_sequences)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "757 + 93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15994"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14582+1412"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
